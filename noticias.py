# -*- coding: utf-8 -*-
"""noticias.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ru2VQAt8DolxLOU7FOE3Y2KrodfIBRht

#Conseguir noticia
"""

!pip3 install newspaper3k
!pip install lxml_html_clean

#@title Extraer noticia de un link
from newspaper import Article
from newspaper import Config # Import the Config class

def obtener_noticia(url):
    # Create a Config object
    config = Config()
    # Set a User-Agent to mimic a browser
    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'

    # Pass the config object to the Article constructor
    articulo = Article(url, config=config, language='es')

    articulo.download()
    articulo.parse()
    return {
        "titulo": articulo.title,
        "texto": articulo.text
    }

#@title Fuentes
FUENTES = {
    # ── Uruguay ──────────────────────────────────────────────
    "Telenoche": {
        "url":  "https://www.telenoche.com.uy/",
        "clase": "nacional"
    },
    "Subrayado": {
        "url":  "https://www.subrayado.com.uy",
        "clase": "nacional"
    },
    "Teledoce": {
        "url":  "https://www.teledoce.com/categoria/telemundo/feed/",
        "clase": "nacional"
    },
    "MontevideoPortal": {
        "url":  "https://www.montevideo.com.uy/",
        "clase": "nacional"
    },
    # ── Internacional ───────────────────────────────────────
    "BBCMundo": {
        "url":  "https://www.bbc.com/mundo",
        "clase": "internacional"
    },
    "CNN": {
        "url":  "https://cnnespanol.cnn.com",
        "clase": "internacional"
    }
}

!pip -q install requests beautifulsoup4 url-normalize

import requests, bs4, urllib.parse, re
from url_normalize import url_normalize

def extraer_links(base):
    headers = {"User-Agent": "Mozilla/5.0 (news-bot)"}
    html = requests.get(base, headers=headers, timeout=20).text
    soup = bs4.BeautifulSoup(html, "html.parser")

    enlaces = set()
    for a in soup.find_all("a", href=True):
        # convierte href relativo → absoluto
        url = urllib.parse.urljoin(base, a["href"])
        url = url_normalize(url.split("?")[0])          # quita parámetros & normaliza
        # filtra solo URLs del dominio y que tengan /nXXXXX (slug típico de noticia)
        if url.startswith(base):
            enlaces.add(url)

    return sorted(enlaces)

for medio, data in FUENTES.items():
    print(f"Enlaces para {medio}:")
    urls = extraer_links(data["url"])
    for url in urls:
        print(url)
    print("-" * 20)

#@title extraer links
def extraer_links(base):
    headers = {"User-Agent": "Mozilla/5.0 (news-bot)"}
    try:
        html = requests.get(base, headers=headers, timeout=20).text
        soup = bs4.BeautifulSoup(html, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"Error fetching URL {base}: {e}")
        return []

    enlaces = set()
    for a in soup.find_all("a", href=True):
        # convierte href relativo → absoluto
        url = urllib.parse.urljoin(base, a["href"])
        url = url_normalize(url.split("?")[0])  # quita parámetros & normaliza

        # filtra solo URLs del dominio y aplica reglas específicas
        if url.startswith(base):
            # Subrayado y Telenoche
            if re.search(r"-n\d+$", url):
                enlaces.add(url)
            # Montevideo Portal
            elif "/Noticias/" in url:
                enlaces.add(url)
            # BBC
            elif "/articles/" in url:
                enlaces.add(url)
            # CNN (assuming 'trax' is part of the news article URL structure for CNN)
            elif "trax" in url:
                 enlaces.add(url)


    return sorted(list(enlaces)) # Convert set to list before sorting

# prompt: Extrae los url de cada medio y despues extrae las noticias dentro de cada medio ademas adjuntando el medio, y si es nacional o internacional la noticia

import pandas as pd

# Initialize an empty list to store news data
all_news = []

# Extract news for each source
for medio, data in FUENTES.items():
    print(f"Processing news for {medio}...")
    urls = extraer_links(data["url"])
    for url in urls:
        try:
            noticia_data = obtener_noticia(url)
            all_news.append({
                "medio": medio,
                "tipo": data["clase"],
                "url": url,
                "titulo": noticia_data["titulo"],
                "texto": noticia_data["texto"]
            })
        except Exception as e:
            print(f"Error processing article {url}: {e}")

# Create a DataFrame from the collected news data
df_news = pd.DataFrame(all_news)

news_count_by_medium = df_news.groupby('medio').size().reset_index(name='cantidad_noticias')

print("Cantidad de noticias por medio:")
news_count_by_medium

"""Hasta acá podemos sacar cualquier noticia de cualquier medio

# Duplicados
"""

!pip install faiss-cpu

#@title Etiquetar noticias
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from typing import List, Dict

# Clasificador de temas
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=0)
etiquetas = ["economía y finanzas", "política", "policial", "deportes", "otros"]

def etiquetar(titulo: str) -> str:
    etiqueta = classifier(titulo, candidate_labels=etiquetas)
    index = etiqueta['scores'].index(max(etiqueta['scores']))
    return etiqueta['labels'][index]

def limpiar_texto(texto: str) -> str:
    return texto.strip().lower()#[:500]

df_news['etiqueta'] = df_news['titulo'].apply(etiquetar)
df_news['texto'] = df_news['texto'].apply(limpiar_texto)

#@title Detectar duplicads

from typing import List, Tuple
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

def detectar_duplicados_titulos(titulos: List[str], modelo: str = "intfloat/multilingual-e5-base",
                                 k: int = 5, umbral: float = 0.87) -> List[Tuple[int, int, float]]:
    """
    Detecta pares de títulos duplicados usando embeddings y FAISS.

    Returns:
        Lista de tuplas (i, j, similitud) donde i y j son índices en la lista de títulos.
    """
    model = SentenceTransformer(modelo)
    titulos_proc = ["query: " + t for t in titulos]
    embeddings = model.encode(titulos_proc, normalize_embeddings=True)

    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)

    D, I = index.search(embeddings, k=k)

    duplicados = []
    for i, (scores, idxs) in enumerate(zip(D, I)):
        for score, j in zip(scores[1:], idxs[1:]):
            if score > umbral:
                duplicados.append((i, int(j), float(score)))
    return duplicados

duplicadas = detectar_duplicados_titulos(df_news['titulo'])

#@title Unificar duplicados
from typing import Dict
import random

def unificar_noticias_con_metadata(noticias: List[Dict], duplicados: List[Tuple[int, int, float]]) -> List[Dict]:
    """
    Une noticias duplicadas y conserva metadata relevante.

    Args:
        noticias: Lista de diccionarios con claves como 'titulo', 'texto', 'url', 'medio', 'tipo', 'etiqueta'.
        duplicados: Lista de tuplas (i, j, score) indicando pares duplicados.

    Returns:
        Lista de noticias fusionadas, eliminando duplicados.
    """
    # 1. Union-Find para agrupar duplicados transitivamente
    parent = list(range(len(noticias)))

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(x, y):
        px, py = find(x), find(y)
        if px != py:
            parent[py] = px

    for i, j, _ in duplicados:
        union(i, j)

    # 2. Agrupar índices por representante
    grupos = defaultdict(list)
    for i in range(len(noticias)):
        root = find(i)
        grupos[root].append(i)

    # 3. Fusionar cada grupo
    noticias_fusionadas = []
    for indices in grupos.values():
        textos = [noticias[i]["texto"] for i in indices]
        medios = [noticias[i]["medio"] for i in indices]
        urls = [noticias[i]["url"] for i in indices]

        fusion_texto = "\n-------\n".join(textos)
        fusion_medio = ", ".join(sorted(set(medios)))
        url_final = random.choice(urls)

        noticia_final = noticias[indices[0]].copy()
        noticia_final["texto"] = fusion_texto
        noticia_final["medio"] = fusion_medio
        noticia_final["url"] = url_final

        noticias_fusionadas.append(noticia_final)

    return noticias_fusionadas


noticias = df_news.to_dict(orient="records")
nuevas_noticias = unificar_noticias_con_metadata(noticias, duplicadas)
df_fusionado = pd.DataFrame(nuevas_noticias)

df_fusionado

"""# Priorizar noticias"""

#@title Conteo Palabras Clave

import re
from collections import Counter

PALABRAS_CLAVE = [
    # Emergencias y sucesos
    "alerta", "accidente", "corte", "evacuación", "muerto", "muerte", "crisis", "incendio",
    "terremoto", "inundación", "desaparecido", "herido", "colapso", "protesta", "paro",
    "violencia", "amenaza", "brote", "emergencia", "tiroteo", "choque", "hospitalizado",
    "urgente", "explosión", "sismo", "detenido", "atentado", "balacera", "pánico",

    # Política y cargos
    "presidente", "vicepresidente", "ministro", "ministra", "senador", "senadora",
    "diputado", "diputada", "intendente", "intendenta", "alcalde", "alcaldesa",
    "gobernador", "gobernadora", "concejal", "concejala", "secretario", "secretaria",
    "subsecretario", "subsecretaria", "parlamento", "congreso", "cámara de diputados",
    "cámara de senadores", "legislador", "legisladora", "funcionario", "autoridad",
    "jefe de estado", "canciller",

    # Tecnología
    "inteligencia artificial", "ai", "chatgpt", "openai", "machine learning",
    "aprendizaje automático", "big data", "modelo de lenguaje", "algoritmo",
    "tecnología", "tecnológico", "tecnológica", "automatización",
    "ciberseguridad", "hackeo", "software", "hardware", "blockchain",
    "criptomoneda", "criptomonedas", "datos", "sistema experto",
    "realidad virtual", "realidad aumentada", "robot", "robótica",
    "programación", "modelo generativo", "deep learning", "red neuronal",
    "cloud computing", "computación en la nube",

    # Conflictos y diplomacia
    "guerra", "conflicto", "invasión", "ataque militar", "bombardeo",
    "frontera", "misil", "soldado", "tropa", "armamento", "defensa", "ejército",
    "fuerzas armadas", "bloqueo", "sanción", "tensión diplomática", "embajada",
    "retirada", "negociación", "acuerdo de paz", "cese al fuego", "otan", "onu",
    "aliado", "enemigo", "ocupación", "rebelde", "resistencia", "militarización",
    "armas químicas", "genocidio", "crímenes de guerra", "intervención extranjera"
]

def contar_coincidencias(texto: str, palabras_clave: list) -> int:
    texto = texto.lower()
    total = 0
    for palabra in palabras_clave:
        ocurrencias = len(re.findall(r'\b' + re.escape(palabra) + r'\b', texto))
        total += ocurrencias
    return total
"""
def agregar_conteo_a_noticias(noticias: list[dict]) -> list[dict]:
    for noticia in noticias:
        texto_completo = (noticia.get("titulo", "") + " " + noticia.get("texto", "")).lower()
        conteo = contar_coincidencias(texto_completo, PALABRAS_CLAVE)
        noticia["conteo_palabras_clave"] = conteo
    return noticias
"""

df_fusionado["palabras_claves"] = df_fusionado["texto"].apply(lambda x: contar_coincidencias(x, PALABRAS_CLAVE))

#@title analisis de sentimiento
from transformers import pipeline

# Cargar el modelo solo una vez
sentiment_analyzer = pipeline("sentiment-analysis",device=0)

"""
def agregar_sentimiento_a_diccionarios(lista_noticias: list[dict]) -> list[dict]:
    for noticia in lista_noticias:
        texto = noticia.get("texto", "").strip()

        if not texto:
            sentimiento = {"label": "NEUTRAL", "score": 0.0}
        else:
            resultado = sentiment_analyzer(texto[:512])[0]
            sentimiento = {
                "label": resultado["label"],
                "score": round(resultado["score"], 3)
            }

        noticia["sentimiento"] = sentimiento

    return lista_noticias
"""

df_fusionado["sentimiento"] = df_fusionado["texto"].apply(lambda x: sentiment_analyzer(x[:512])[0]['score'])

df_fusionado

df_fusionado.columns

#@title Score de noticias

def score(news):
    s  = 0
    s += {"nacional":0, "internacional":1}[news["tipo"]]
    if news["etiqueta"] in {"clima", "salud"}:  s += 1
    if news["sentimiento"] > 0.8:  s += 10*news["sentimiento"]
    s += news["palabras_claves"]
    return s
df_fusionado["score"] = df_fusionado.apply(score, axis=1)

internacionales = df_fusionado[df_fusionado['tipo']=='internacional'].sort_values(by="score", ascending=False).head(5)
nacionales = df_fusionado[df_fusionado['tipo']=='nacional'].sort_values(by="score", ascending=False).head(10)

df_final = pd.concat([nacionales, internacionales])
df_final = df_final.sort_values(by="score", ascending=False)
df_final

"""# Resumir noticia
(Creo que lo vamos a hacer en Zapier)

import openai
import os

# Replace 'YOUR_ACTUAL_OPENAI_API_KEY' with your actual key

openai_api_key = os.environ["OPENAI_API_KEY"]

client = openai.OpenAI()


def resumir_noticia(texto):
    prompt = f"Resume la siguiente noticia en un lenguaje claro y atractivo para una persona mayor:\n\n{texto}"
    # Use the new API syntax for creating chat completions
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=400
    )
    # Access the content from the response object using dot notation
    return response.choices[0].message.content"

#Integracion con Zapier
"""

# prompt: pega las noticias de df final de la siguiente forma
# Titulo ...
# texto ...
# Medio
# url
# -------
# Otra noticias
# en un gran string

noticias_string = ""
for index, row in df_final.iterrows():
    noticias_string += f"Titulo: {row['titulo']}\n"
    noticias_string += f"Texto: {row['texto']}\n"
    noticias_string += f"Medio: {row['medio']}\n"
    noticias_string += f"URL: {row['url']}\n"
    noticias_string += "-------\n"

noticias_string

import requests

def enviar_resumen_por_zapier(texto: str):
    url = "https://hooks.zapier.com/hooks/catch/23067394/2vra34l/"
    payload = {
        "mensaje": texto
    }
    try:
        r = requests.post(url, json=payload, timeout=10)
        r.raise_for_status()
        print("✅ Resumen enviado a Zapier")
    except requests.exceptions.RequestException as e:
        print(f"❌ Error enviando a Zapier: {e}")

enviar_resumen_por_zapier(noticias_string)

